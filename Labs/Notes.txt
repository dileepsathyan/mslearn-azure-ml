
# Resouces
- Workspace
- Compute resources
- Datastores


# Assets
- Models
- Environments
- Data
- Components
- Jobs
- Pipelines
- Endpoints



# Datastores
- workspaceartifactstore: Connects to the azureml container of the Azure Storage account created with the workspace. 
                            Used to store compute and experiment logs when running jobs.

- workspaceworkingdirectory: Connects to the file share of the Azure Storage account created with the workspace used by 
                            the Notebooks section of the studio. Whenever you upload files or folders to access from a compute instance, 
                            the files or folders are uploaded to this file share.

- workspaceblobstore: Connects to the Blob Storage of the Azure Storage account created with the workspace. 
                            Specifically the azureml-blobstore-... container. Set as the default datastore, 
                            which means that whenever you create a data asset and upload data, you store the data in this container.

- workspacefilestore: Connects to the file share of the Azure Storage account created with the workspace. 
                            Specifically the azureml-filestore-... file share.



# Command: Execute a single script.
# Sweep: Perform hyperparameter tuning when executing a single script.
# Pipeline: Run a pipeline consisting of multiple scripts or components.




# For Azure Machine Learning to connect to your data, you need to prefix the URI with the appropriate protocol. 
    There are three common protocols when working with data in the context of Azure Machine Learning:

    - http(s): Use for data stores publicly or privately in an Azure Blob Storage or publicly available http(s) location.
    - abfs(s): Use for data stores in an Azure Data Lake Storage Gen 2.
    - azureml: Use for data stored in a datastore.



# Datastores: The benefits of using datastores are:
    - Provides easy-to-use URIs to your data storage.
    - Facilitates data discovery within Azure Machine Learning.
    - Securely stores connection information, without exposing secrets and keys to data scientists.



# Data Assets: The benefits of using data assets are:
    - Share and reuse data with other members of the team such that they don't need to remember file locations.
    - Seamlessly access data during model training (on any supported compute type) without worrying about connection strings or data paths.
    - Version the metadata of the data asset.


    # 3 types of data assets:
        - URI file: Points to a specific file.
        - URI folder: Points to a folder.
        - MLTable: Points to a folder or file, and includes a schema to read as tabular data.




# Compute Types:

    * Compute instance: Behaves similarly to a virtual machine and is primarily used to run notebooks. It's ideal for experimentation.
    * Compute clusters: Multi-node clusters of virtual machines that automatically scale up or down to meet demand. A cost-effective way to run scripts that need to process large volumes of data. Clusters also allow you to use parallel processing to distribute the workload and reduce the time it takes to run a script.
    * Kubernetes clusters: Cluster based on Kubernetes technology, giving you more control over how the compute is configured and managed. You can attach your self-managed Azure Kubernetes (AKS) cluster for cloud compute, or an Arc Kubernetes cluster for on-premises workloads.
    * Attached compute: Allows you to attach existing compute like Azure virtual machines or Azure Databricks clusters to your workspace.
    * Serverless compute: A fully managed, on-demand compute you can use for training jobs.
 



# Custom logging with MLflow
    * mlflow.log_param(): Logs a single key-value parameter. Use this function for an input parameter you want to log.
    * mlflow.log_metric(): Logs a single key-value metric. Value must be a number. Use this function for any output you want to store with the run.
    * mlflow.log_artifact(): Logs a file. Use this function for any plot you want to log, save as image file first.
    * mlflow.log_model(): Logs a model. Use this function to create an MLflow model, which may include a custom signature, environment, and input examples.





# Search Space: The set of hyperparameter values tried during hyperparameter tuning
    * Discrete: 
        - Python list (Choice(values=[10,20,30])), 
        - a range (Choice(values=range(1,10))), or 
        - an arbitrary set of comma-separated values (Choice(values=(30,50,100)))

        - QUniform(min_value, max_value, q): Returns a value like round(Uniform(min_value, max_value) / q) * q
        - QLogUniform(min_value, max_value, q): Returns a value like round(exp(Uniform(min_value, max_value)) / q) * q
        - QNormal(mu, sigma, q): Returns a value like round(Normal(mu, sigma) / q) * q
        - QLogNormal(mu, sigma, q): Returns a value like round(exp(Normal(mu, sigma)) / q) * q

    * Continuous:
        - Uniform(min_value, max_value): Returns a value uniformly distributed between min_value and max_value
        - LogUniform(min_value, max_value): Returns a value drawn according to exp(Uniform(min_value, max_value)) so that the logarithm of the return value is uniformly distributed
        - Normal(mu, sigma): Returns a real value that's normally distributed with mean mu and standard deviation sigma
        - LogNormal(mu, sigma): Returns a value drawn according to exp(Normal(mu, sigma)) so that the logarithm of the return value is normally distributed






# Sweep jobs: To run a sweep job the training script must have the following 2 things:
    1. Include an argument for each hyperparameter you want to vary.
    2. Log the target performance metric with MLflow. A logged metric enables the sweep job to evaluate 
        the performance of the trials it initiates, and identify the one that produces the best performing model.







# Early termination policy for Sweep jobs:
There are 2 main parameters when you choose to use an early termination policy:

    - evaluation_interval: Specifies at which interval you want the policy to be evaluated. 
                           Every time the primary metric is logged for a trial counts as an interval.
    - delay_evaluation: Specifies when to start evaluating the policy. This parameter allows for at least 
                        a minimum of trials to complete without an early termination policy affecting them.


    There are three options for early termination:

    * Bandit policy: Uses a slack_factor (relative) or slack_amount(absolute). Any new model must perform 
                        within the slack range of the best performing model.
    * Median stopping policy: Uses the median of the averages of the primary metric. 
                        Any new model must perform better than the median.
    * Truncation selection policy: Uses a truncation_percentage, which is the percentage of lowest performing trials. 
                        Any new model must perform better than the lowest performing trials.






# Components: consists of 3 parts.
    - Metadata: Includes the component's name, version, etc.
    - Interface: Includes the expected input parameters (like a dataset or hyperparameter) and expected output (like metrics and artifacts).
    - Command, code and environment: Specifies how to run the code.




# Pipeline
    - The pipeline is built by defining the function pipeline_function_name.
    - The pipeline function expects pipeline_job_input as the overall pipeline input.
    - The first pipeline step requires a value for the input parameter input_data. The value for the input will be the value of pipeline_job_input.
    - The first pipeline step is defined by the loaded component for prep_data.
    - The value of the output_data of the first pipeline step is used for the expected input training_data of the second pipeline step.
    - The second pipeline step is defined by the loaded component for train_model and results in a trained model referred to by model_output.
    - Pipeline outputs are defined by returning variables from the pipeline function. There are two outputs:
        - pipeline_job_transformed_data with the value of prep_data.outputs.output_data
        - pipeline_job_trained_model with the value of train_model.outputs.model_output






# MLflow Models: There are three types of models you can register:

    - MLflow: Model trained and tracked with MLflow. Recommended for standard use cases.
    - Custom: Model type with a custom standard not currently supported by Azure Machine Learning.
    - Triton: Model type for deep learning workloads. Commonly used for TensorFlow and PyTorch model deployments.



# Endpoints: there are two types of online endpoints:

    - Managed online endpoints: AML manages all the underlying infrastructure.
    - Kubernetes online endpoints: Users manage the Kubernetes cluster which provides the necessary infrastructure.




# Deploying a model to an endpoint: 

To deploy a model, you must have:
    - Model files stored on local path or registered model.
    - A scoring script.
    - An execution environment.

You need to specify 4 things:
    - Model assets like the model pickle file, or a registered model in the Azure Machine Learning workspace.
    - Scoring script that loads the model.
    - Environment which lists all necessary packages that need to be installed on the compute of the endpoint.
    - Compute configuration incl. the needed compute size and scale settings to ensure you can handle the amount of requests the endpoint will receive.


The scoring script needs to include two functions:
    - init(): Called when the service is initialized.
    - run(): Called when new data is submitted to the service.




=======================================================================================================================================================================================


# Responsible AI

    * Fairness
    * Reliability & Safety
    * Privacy & Security
    * Inclusiveness
    * Transparency
    * Accountability 


# Model Benchmarks
    * Accuracy - Compares model-generated text with correct answer according to the dataset. Result is one if generated text matches the answer exactly, and zero otherwise.
    * Coherence - Measures whether the model output flows smoothly, reads naturally, and resembles human-like language.
    * Fluency - Assesses how well the generated text adheres to grammatical rules, syntactic structures, and appropriate usage of vocabulary, resulting in linguistically correct and natural-sounding responses.
    * Groundedness -	Measures alignment between the model's generated answers and the input data.
    * GPT Similarity - Quantifies the semantic similarity between a ground truth sentence (or document) and the prediction sentence generated by an AI model.
    * Quality index - A comparative aggregate score between 0 and 1, with better-performing models scoring a higher value
    * Cost - The cost of using the model based on a price-per-token. Cost is a useful metric with which to compare quality, enabling you to determine an appropriate tradeoff for your needs.



# LLM Deveopment Lifecycle
    1. Initialization: Define the use case and design the solution.
        - Define the objective
        - Collect a sample dataset
        - Build a basic prompt
        - Design the flow
    
    2. Experimentation: Develop a flow and test with a small dataset.
        - run the flow against a sample dataset. 
        - evaluate the prompt's performance. 
        - if satisfied with the result, evaluation and refinement. 
        - if not, modify the flow by changing the prompt or flow itself.
    
    3. Evaluation and refinement: Assess the flow with a larger dataset.

    4. Production: Deploy and monitor the flow and application.
        - Optimize the flow that classifies incoming articles for efficiency and effectiveness.
        - Deploy your flow to an endpoint. When you call the endpoint, the flow is triggered to run and the desired output is generated.
        - Monitor the performance of your solution by collecting usage data and end-user feedback. By understanding how the application performs, you can improve the flow whenever necessary.


# Prompt Flow: The 3 common tools are:
    - LLM tool: Enables custom prompt creation utilizing Large Language Models.
    - Python tool: Allows the execution of custom Python scripts.
    - Prompt tool: Prepares prompts as strings for complex scenarios or integration with other tools.


# Types of flows: 3 types of flows you can create with prompt flow:
    - Standard flow: Ideal for general LLM-based application development, offering a range of versatile tools.
    - Chat flow: Designed for conversational applications, with enhanced support for chat-related functionalities.
    - Evaluation flow: Focused on performance evaluation, allowing the analysis and improvement of models or applications through feedback on previous runs.



# Runtimes: are a combination of a compute instance providing the necessary compute resources, 
    and an environment specifying the necessary packages and libraries that need to be installed before being able to run the flow.



# Search Index: There are several ways that information can be queried in an index:
    - Keyword search: Identifies relevant documents or passages based on specific keywords or terms provided as input.
    - Semantic search: Retrieves documents or passages by understanding the meaning of the query and matching it with semantically related content rather than relying solely on exact keyword matches.
    - Vector search: Uses mathematical representations of text (vectors) to find similar documents or passages based on their semantic meaning or context.
    - Hybrid search: Combines any or all of the other search techniques. Queries are executed in parallel and are returned in a unified result set.




# Some considerations you can take into account when deciding on a foundation model before fine-tuning are:
    - Model capabilities: Evaluate the capabilities of the foundation model and how well they align with your task. For example, a model like BERT is better at understanding short texts.
    - Pretraining data: Consider the dataset used for pretraining the foundation model. For example, GPT-2 is trained on unfiltered content from the internet that can result in biases.
    - Limitations and biases: Be aware of any limitations or biases that might be present in the foundation model.
    - Language support: Explore which models offer the specific language support or multilingual capabilities that you need for your use case.





# 4 Stage Process for Gen AI solution:
    1. Map potential harms that are relevant to your planned solution.
        - Identify potential harms
        - Prioritize identified harms
        - Test and verify the prioritized harms
        - Document and share the verified harms
    
    2. Measure the presence of these harms in the outputs generated by your solution.
    3. Mitigate the harms at multiple layers in your solution to minimize their presence and impact, and ensure transparent communication about potential risks to users.
        Appy mitigations steps in 4 layers:
        - Model
        - Safety system: platform level config (safe/low/med/high)
        - System message & grounding
        - User experience

    4. Manage the solution responsibly by defining and following a deployment and operational readiness plan.




# Common compliance reviews
    1. Legal
    2. Provacy
    3. Security
    4. Accessibility



# Microsoft Foundry Content Safety features:
    - Prompt shields - Scans for the risk of user input attacks on language models
    - Groundedness detection - Detects if text responses are grounded in a user's source content
    - Protected material detection - Scans for known copyrighted content
    - Custom categories - Define custom categories for any new or emerging patterns


# Common NLP metrics: to quantify the level of overlap in the model-generated response and the ground truth
    * BLEU: Bilingual Evaluation Understudy metric
    * METEOR: Metric for Evaluation of Translation with Explicit Ordering
    * ROUGE: Recall-Oriented Understudy for Gisting Evaluation